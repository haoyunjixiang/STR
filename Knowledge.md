## 各层 FLOPs 和参数量 paras 的计算
### 参数量
(K*K*Cin)*Cout + Cout  = (K*K*Cin+1)*Cout 
### 计算量
计算量在参数量的基础上再乘以 输出特征图的大小，因为输出的每个点对应参数的计算
(K*K*Cin+1)*Cout*H*W

## 感受野大小的计算
*type* | *size* | *stride* |
:---: | :---: |:--- |
conv1 | 3 | 1 |
pool1 | 3 | 2 |
conv2 | 3 | 1 |
pool2 | 2 | 2 |
conv3 | 3 | 1 |
conv4 | 3 | 1 |
pool3 | 2 | 2 |

从pool3算起，其一个点对应2 * 2，那么conv4的输出为2 * 2的话，conv4的输入为4 * 4，以此类推：
conv3为 6 * 6<br>
pool2为 12 * 12<br>
conv2为 14 * 14<br>
pool1为 28 * 28<br>
conv1为 30 * 30<br>

RF = 1<br>
for layer from (down to top):<br>
&nbsp;&nbsp;&nbsp;&nbsp;RF = (RF-1)*stride + fsize

## CV中的卷积操作是互相关还是卷积
卷积层后面连接的是池化层， 也就是说把卷积结果得到的矩阵中，
选取矩阵中数值最大的元素作为保留，矩阵中其余元素一律删除。
卷积结果中的最大值 与 互相关结果矩阵中的最大值是一样的，
CNN中使用卷积或互相关，贡献是一致的，都是获取像素最大的那个值，
因此可以使用卷积，也可以使用互相关， 但是为了代码的高效，直接使用“互相关”即可。

## batch size 和 learning rate 的关系
1. 增加batch size会使得梯度更准确，但也会导致variance变小，可能会使模型陷入局部最优；
2. 因此增大batch size通常要增大learning rate，比如batch size增大m倍，lr增大m倍或者sqrt(m)倍，但并不固定；

## 类别不均衡
1. 重采样 一般提升低频类别
2. 重加权 如Focalloss
因此可以通过设定a的值（一般而言假如1这个类的样本数比-1这个类的样本数多很多，那么a会取0到0.5来增加-1这个类的样本的权重）来控制正负样本对总的loss的共享权重。
   
![img.png](img/focal1.png)
   
显然前面的公式3虽然可以控制正负样本的权重，但是没法控制容易分类和难分类样本的权重，于是就有了focal loss：

![img_1.png](img/focal2.png)

这里介绍下focal loss的两个重要性质：1、当一个样本被分错的时候，pt是很小的（请结合公式2，比如当y=1时，p要小于0.5才是错分类，此时pt就比较小，反之亦然），因此调制系数就趋于1，也就是说相比原来的loss是没有什么大的改变的。当pt趋于1的时候（此时分类正确而且是易分类样本），调制系数趋于0，也就是对于总的loss的贡献很小。2、当γ=0的时候，focal loss就是传统的交叉熵损失，当γ增加的时候，调制系数也会增加。
focal loss的两个性质算是核心，其实就是用一个合适的函数去度量难分类和易分类样本对总的损失的贡献。
作者在实验中采用的是公式5的focal loss（结合了公式3和公式4，这样既能调整正负样本的权重，又能控制难易分类样本的权重）：

![img_2.png](img/focal3.png)

一般而言当γ增加的时候，a需要减小一点。（实验中γ=2，a=0.25的效果最好）

## 迁移学习都有哪些方式，怎么在网络训练中使用
1. 预训练模型方法  固定部分权值，微调更新末端权值

## 权重初始化方法有哪些
1. 把w初始化为0
2. 对w随机初始化（正态分布）
3. Xavier initialization（适用于sigmod,tanh函数）
   ![img.png](img/weight_init1.png)
   ![img_1.png](img/weight_init2.png)
4. He initialization（MSRA 适用于relu激活函数）
   ![img_2.png](img/weight_init3.png)
   ![img_3.png](img/weight_init4.png)

## 计算机视觉中的注意力机制
参考：https://zhuanlan.zhihu.com/p/146130215

attention机制可以它认为是一种资源分配的机制，可以理解为对于原本平均分配的资源根据attention对象的重要程度重新分配资源，重要的单位就多分一点，不重要或者不好的单位就少分一点，在深度神经网络的结构设计中，attention所要分配的资源基本上就是权重了。

视觉注意力分为几种，核心思想是基于原有的数据找到其之间的关联性，然后突出其某些重要特征，有通道注意力，像素注意力，多阶注意力等，也有把NLP中的自注意力引入。

## onehot 编码可能会遇到的问题
onehot主要针对无序分类编码进行转换，常用的距离或相似度的计算都是在欧式空间的相似度计算，比如计算余弦相似性，就是基于欧式空间。将离散型特征使用one-hot编码，确实会让特征之间的距离计算更加合理。
比如，有一个离散型特征，代表工作类型，该离散型特征，共有三个取值，不使用one-hot编码，其表示分别是x_1 = 1, x_2 = 2, x_3 = 3。两个工作之间的距离是，d(x_1, x_2)= 1, d(x_2, x_3) = 1, d(x_1, x_3) = 2。那么x_1和x_3工作之间就越不相似吗？显然这样的表示，计算出来的特征的距离是不合理。那如果使用one-hot编码，则得到x_1 = (1, 0, 0), x_2 = (0, 1, 0), x_3 = (0, 0, 1)，那么两个工作之间的距离就都是sqrt(2).即每两个工作之间的距离是一样的，显得更合理。

onehot编码带来的特征维度不同：

例如有5个特征，其中四个连续一个离散，这种情况下用onehot编码，相当于该离散特征占了4维。

onehot解决的是从特征角度降低问题非线性的问题。虽然说连续特征离散后维度变高，可以认为讲隐藏在连续特征中的信息为模型学习摊开了，模型可利用的有效信息量更大。可以使用条件熵分析下就可以得到结论。gbdt类的模型的学习过程就是不断找到具有最大收益的特征并进一步找到这个特征在样本上的分裂值，所以它需要单个特征的variation要大一些，由于在树上不同的分裂点可能会对同一个特征的不同值做分裂，所以即使这个特征与label表现出非线性关系，他也能学到知识。反之，如果经过onehot后的特征交给模型学习，特征在整个样本集上就两个取值，模型学到的信息较少。

One hot编码维度过高解决方法：
1. 根据类别特征的意义进行合并（分桶）
2. 将类别按频次排序，频次特别低的一部分合并
3. 特征哈希
4. PCA降维
5. 按照该特征对应目标值进行合并
6. 使用每个分类对应目标变量均值+偏差，或出现频数代替

## 分类问题有哪些评价指标？每种的适用场景
1. Accuracy  (Area Under Curve)
2. precision
3. recall
4. F1 score
5. ROC
6. PR （Precision-Recall）曲线

## 为什么要用 F1 score
若只用准确率来表示癌症病人的预测，模型只输出健康，也能达到很高的准确率，这显示不合理。
precision = TP / (TP + FP)<br>
recall = TP / (TP + FN)<br>
F1 = 2 * precision * recall / (precision + recall)

## L1 正则化与 L2 正则化的区别
正则化：为了限制模型的参数，防止模型过拟合而加在损失函数后面的一项。

L1正则化：各个参数的绝对值之和，能够产生稀疏矩阵。 因为最优的参数值很大概率出现在坐标轴上，这样就会导致某一维的权重为0 ，产生稀疏权重矩阵<br>
L2正则化：各个参数的平方和。能够使各个参数趋于0.

1. 为什么参数越小代表模型越简单？
　　越是复杂的模型，越是尝试对所有样本进行拟合，包括异常点。这就会造成在较小的区间中产生较大的波动，这个较大的波动也会反映在这个区间的导数比较大。
　　只有越大的参数才可能产生较大的导数。因此参数越小，模型就越简单。

2. 实现参数的稀疏有什么好处？
　　因为参数的稀疏，在一定程度上实现了特征的选择。一般而言，大部分特征对模型是没有贡献的。这些没有用的特征虽然可以减少训练集上的误差，但是对测试集的样本，反而会产生干扰。稀疏参数的引入，可以将那些无用的特征的权重置为0.

## CNN分类网络的演变脉络
参考：https://www.cnblogs.com/lyp1010/p/12562271.html

## ResNet 流行的原因
1. 通过引入残差模块，解决网络退化的问题，使得网络可以更深，学习到更好的表示
2. 梯度增大，快速收敛，加快训练速度。

ResNet_v2 与 v1 的最大区别就是 v2 的 BN 和 ReLU 是在卷积之前使用的，好处：

1. 反向传播基本符合假设，信息传递无阻碍；
2. BN 层作为 pre-activation，起到了正则化的作用

## Inception v1-v4的区别与改进
![img.png](img/inception.png)

v1:多个不同size的卷积核能够增强网络的适应能力

v2:学习了VGG使用两个3*3的卷积代替5*5的大卷积，在降低参数的同时建立了更多的非线性变换，使得CNN对特征的学习能力更强。

v3:

## MobileNet，ShuffleNet
1. mobilenet
![img.png](img/mobilenet.png)
2. shufflenet
   <br>shuffle-net 这个网络模型，是利用了 group 卷积的概念，与 depth-wise 有点像，只不过，depth-wise 是 feature map 一 一对应的，而 group 卷积是将每个卷积层的 feature map 分成几个组，每个组之间没有交叉，不过组内的 feature map 做卷积的时候和常规的卷积运算是一样的，所以 group 卷积的复杂度应该是介于常规卷积核 depth-wise 卷积之间的，shuffle-net 的创新之处在于，group 卷积之后，为了增加组与组之间的 feature map的通信，提出了一个 shuffle channel 的技术，就是将 group 卷积之后 feature map 打乱，乱序连接到下一层，如下图所示：
![img_1.png](img/shufflenet.png)
   
## FCN 和 UNET
1. FCN
   + 全卷积 区别于分类网络的全连接
   + 上采样 为了与标签一一对应进行训练
   + 跳跃连接 将浅层的位置信息和深层的语义信息结合起来，得到更佳鲁棒的结果
   ![img_2.png](img/fcn.png)
2. UNET
   + U型结构 编码器-解码器结构 即将图像->高语义feature map的过程看成编码器，高语义->像素级别的分类score map的过程看作解码器）进行了加卷积加深处理，FCN只是单纯的进行了上采样。
   + Skip connection：两者都用了这样的结构
   + 联合：在FCN中，Skip connection的联合是通过对应像素的求和，而U-Net则是对其的channel的concat过程。
   ![img_2.png](img/unet.png)

## 兩阶段目标检测网络的发展脉络及各自的贡献与特点
参考：https://blog.csdn.net/qq_38144185/article/details/115699848
1. RCNN
   + 候选框 + SVM
   + 首次使用CNN提取特征代替手工HOG，SIFT特征。
2. SPP-net
   + 进行一次CNN操作，在特征图上提取候选框
   + 进行金字塔池化，得到特征向量
3. Fast-RCNN
   + 添加ROI pooling
   + 使用softmax
   + 多任务损失函数（分类+回归）
4. Faster-RCNN
   + 提出RPN生成候选框，提出了anchor
   + RPN和检测共享特征
   

## 解释 ROI Pooling 和 ROI Align 的区别
都是将候选框池化到固定尺寸
1. ROI Pooling
   + 使用两次量化操作进行池化：第一次如665/32 x 665/32 = 20.78 x 20.78   20 /7 x 20/7 = 2.86 x 2.86
   + 使用线性插值进行池化。