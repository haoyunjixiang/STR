## 各层 FLOPs 和参数量 paras 的计算
### 参数量
(K*K*Cin)*Cout + Cout  = (K*K*Cin+1)*Cout 
### 计算量
计算量在参数量的基础上再乘以 输出特征图的大小，因为输出的每个点对应参数的计算
(K*K*Cin+1)*Cout*H*W

## 感受野大小的计算
*type* | *size* | *stride* |
:---: | :---: |:--- |
conv1 | 3 | 1 |
pool1 | 3 | 2 |
conv2 | 3 | 1 |
pool2 | 2 | 2 |
conv3 | 3 | 1 |
conv4 | 3 | 1 |
pool3 | 2 | 2 |

从pool3算起，其一个点对应2 * 2，那么conv4的输出为2 * 2的话，conv4的输入为4 * 4，以此类推：
conv3为 6 * 6<br>
pool2为 12 * 12<br>
conv2为 14 * 14<br>
pool1为 28 * 28<br>
conv1为 30 * 30<br>

RF = 1<br>
for layer from (down to top):<br>
&nbsp;&nbsp;&nbsp;&nbsp;RF = (RF-1)*stride + fsize

## CV中的卷积操作是互相关还是卷积
卷积层后面连接的是池化层， 也就是说把卷积结果得到的矩阵中，
选取矩阵中数值最大的元素作为保留，矩阵中其余元素一律删除。
卷积结果中的最大值 与 互相关结果矩阵中的最大值是一样的，
CNN中使用卷积或互相关，贡献是一致的，都是获取像素最大的那个值，
因此可以使用卷积，也可以使用互相关， 但是为了代码的高效，直接使用“互相关”即可。

## batch size 和 learning rate 的关系
1. 增加batch size会使得梯度更准确，但也会导致variance变小，可能会使模型陷入局部最优；
2. 因此增大batch size通常要增大learning rate，比如batch size增大m倍，lr增大m倍或者sqrt(m)倍，但并不固定；

## 类别不均衡
1. 重采样 一般提升低频类别
2. 重加权 如Focalloss
因此可以通过设定a的值（一般而言假如1这个类的样本数比-1这个类的样本数多很多，那么a会取0到0.5来增加-1这个类的样本的权重）来控制正负样本对总的loss的共享权重。
   
![img.png](img/focal1.png)
   
显然前面的公式3虽然可以控制正负样本的权重，但是没法控制容易分类和难分类样本的权重，于是就有了focal loss：

![img_1.png](img/focal2.png)

这里介绍下focal loss的两个重要性质：1、当一个样本被分错的时候，pt是很小的（请结合公式2，比如当y=1时，p要小于0.5才是错分类，此时pt就比较小，反之亦然），因此调制系数就趋于1，也就是说相比原来的loss是没有什么大的改变的。当pt趋于1的时候（此时分类正确而且是易分类样本），调制系数趋于0，也就是对于总的loss的贡献很小。2、当γ=0的时候，focal loss就是传统的交叉熵损失，当γ增加的时候，调制系数也会增加。
focal loss的两个性质算是核心，其实就是用一个合适的函数去度量难分类和易分类样本对总的损失的贡献。
作者在实验中采用的是公式5的focal loss（结合了公式3和公式4，这样既能调整正负样本的权重，又能控制难易分类样本的权重）：

![img_2.png](img/focal3.png)

一般而言当γ增加的时候，a需要减小一点。（实验中γ=2，a=0.25的效果最好）

## 迁移学习都有哪些方式，怎么在网络训练中使用
1. 预训练模型方法  固定部分权值，微调更新末端权值

## 权重初始化方法有哪些
1. 把w初始化为0
2. 对w随机初始化（正态分布）
3. Xavier initialization（适用于sigmod,tanh函数）
   ![img.png](img/weight_init1.png)
   ![img_1.png](img/weight_init2.png)
4. He initialization（MSRA 适用于relu激活函数）
   ![img_2.png](img/weight_init3.png)
   ![img_3.png](img/weight_init4.png)

## 计算机视觉中的注意力机制
参考：https://zhuanlan.zhihu.com/p/146130215

attention机制可以它认为是一种资源分配的机制，可以理解为对于原本平均分配的资源根据attention对象的重要程度重新分配资源，重要的单位就多分一点，不重要或者不好的单位就少分一点，在深度神经网络的结构设计中，attention所要分配的资源基本上就是权重了。

视觉注意力分为几种，核心思想是基于原有的数据找到其之间的关联性，然后突出其某些重要特征，有通道注意力，像素注意力，多阶注意力等，也有把NLP中的自注意力引入。

## onehot 编码可能会遇到的问题
onehot主要针对无序分类编码进行转换，常用的距离或相似度的计算都是在欧式空间的相似度计算，比如计算余弦相似性，就是基于欧式空间。将离散型特征使用one-hot编码，确实会让特征之间的距离计算更加合理。
比如，有一个离散型特征，代表工作类型，该离散型特征，共有三个取值，不使用one-hot编码，其表示分别是x_1 = 1, x_2 = 2, x_3 = 3。两个工作之间的距离是，d(x_1, x_2)= 1, d(x_2, x_3) = 1, d(x_1, x_3) = 2。那么x_1和x_3工作之间就越不相似吗？显然这样的表示，计算出来的特征的距离是不合理。那如果使用one-hot编码，则得到x_1 = (1, 0, 0), x_2 = (0, 1, 0), x_3 = (0, 0, 1)，那么两个工作之间的距离就都是sqrt(2).即每两个工作之间的距离是一样的，显得更合理。

onehot编码带来的特征维度不同：

例如有5个特征，其中四个连续一个离散，这种情况下用onehot编码，相当于该离散特征占了4维。

onehot解决的是从特征角度降低问题非线性的问题。虽然说连续特征离散后维度变高，可以认为讲隐藏在连续特征中的信息为模型学习摊开了，模型可利用的有效信息量更大。可以使用条件熵分析下就可以得到结论。gbdt类的模型的学习过程就是不断找到具有最大收益的特征并进一步找到这个特征在样本上的分裂值，所以它需要单个特征的variation要大一些，由于在树上不同的分裂点可能会对同一个特征的不同值做分裂，所以即使这个特征与label表现出非线性关系，他也能学到知识。反之，如果经过onehot后的特征交给模型学习，特征在整个样本集上就两个取值，模型学到的信息较少。

One hot编码维度过高解决方法：
1. 根据类别特征的意义进行合并（分桶）
2. 将类别按频次排序，频次特别低的一部分合并
3. 特征哈希
4. PCA降维
5. 按照该特征对应目标值进行合并
6. 使用每个分类对应目标变量均值+偏差，或出现频数代替

## 为什么要用 F1 score
若只用准确率来表示癌症病人的预测，模型只输出健康，也能达到很高的准确率，这显示不合理。
precision = TP / (TP + FP)<br>
recall = TP / (TP + FN)<br>
F1 = 2 * precision * recall / (precision + recall)


