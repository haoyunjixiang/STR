## BN回顾
BN全称Batch Normalization。是为了对网络每层的数据进行归一化。
## 网络的层数据的归一化
机器学习里有个很重要的假设，训练数据和测试数据是同分布的。batchsize越大越能反应整体分布，训练效果越好。
但是网络内部，每次参数更新，都会导致每一层的参数分布发生变化，就会导致模拟效果差，收敛速度慢。
而BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。
近似白化预处理：
![img_16.png](../img/BN1.png)
上面是各神经元的平均值，下面是各神经元输入值的标准差。
## 直接归一化的问题
直接归一化强制的转换会导致数据的分布特征发生破坏，比如本来数据大部分都在0的右边，经过RELU激活函数以后大部分会被激活，如果直接强制归一化，那么就会有大多数的数据无法激活了，这样学习到的特征就被破坏掉了。
引出变换重构：
![img_16.png](../img/BN2.png)
当gama等于训练数据方差，beta等于训练数据期望时，可以恢复原始数据的特征分布。

BN的好处：
1. 防止网络梯度消失：这个要结合sigmoid函数进行理解
2. 加速训练，也允许更大的学习率：输出分布向着激活函数的上下限偏移，带来的问题就是梯度的降低，（比如说激活函数是sigmoid），通过normalization，数据在一个合适的分布空间，经过激活函数，仍然得到不错的梯度。梯度好了自然加速训练。
3. 降低参数初始化敏感：以往模型需要设置一个不错的初始化才适合训练，加了BN就不用管这些了，现在初始化方法中随便选择一个用，训练得到的模型就能收敛。
4. 提高网络泛化能力防止过拟合：所以有了BN层，可以不再使用L2正则化和dropout。可以理解为在训练中，BN的使用使得一个mini-batch中的所有样本都被关联在了一起，因此网络不会从某一个训练样本中生成确定的结果。
5. 可以把训练数据彻底打乱（防止每批训练的时候，某一个样本都经常被挑选到，文献说这个可以提高1%的精度）。
